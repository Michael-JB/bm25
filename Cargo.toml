[package]
name = "bm25"
version = "2.1.1"
description = "BM25 embedder, scorer, and search engine"
repository = "https://github.com/Michael-JB/bm25"
license = "MIT"
edition = "2021"
keywords = ["bm25", "sparse", "embed", "search", "nlp"]
categories = ["algorithms", "text-processing"]
authors = ["Michael <michael@ramyun.co.uk>"]
exclude = [
  ".github/**",
  "/snapshots/**",
  "/data/**",
]

[package.metadata.docs.rs]
all-features = true

[features]
default = ["default_tokenizer"]

# The default tokenizer is a good choice for most use-cases. It normalizes unicode, splits unicode
# word boundaries, removes stop words, and stems the remaining words.
default_tokenizer = ["dep:cached", "dep:stop-words", "dep:rust-stemmers", "dep:deunicode", "dep:unicode-segmentation"]

# With language detection enabled, you can configure the default tokenizer to detect the
# language of the input text.
language_detection = ["dep:whichlang", "default_tokenizer"]

# With parallelism enabled, batch fitting jobs happen in parallel.
parallelism = ["dep:rayon"]

[dependencies]
cached = { version = "0.54.0", optional = true }
deunicode = { version = "1.6.0", optional = true }
fxhash = "0.2.1"
rayon = { version = "1.10.0", optional = true }
rust-stemmers = { version = "1.2.0", optional = true }
stop-words = { version = "0.8.0", optional = true, default-features = false, features = ["nltk"] }
unicode-segmentation = { version = "1.12.0", optional = true }
whichlang = { version = "0.1.0", optional = true }

[dev-dependencies]
csv = "1.3.1"
divan = "0.1.17"
insta = "1.41.1"
rayon = "1.10.0"

[[bench]]
name = "embedder"
harness = false

[[bench]]
name = "search"
harness = false
